{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maria.selezniova/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate bag of word representations for groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "744515it [00:02, 366223.63it/s]\n"
     ]
    }
   ],
   "source": [
    "user_id_mapping = {}\n",
    "\n",
    "with open(\"../csv/user_id_mapping.csv\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split(',')\n",
    "        user_id_mapping[int(parts[0])] = int(parts[1])\n",
    "\n",
    "\n",
    "with open('../csv/publications.csv') as fin:\n",
    "    fin.readline()\n",
    "    docs = []\n",
    "    ids = []\n",
    "    r = 0\n",
    "    for line in tqdm(fin):\n",
    "        parts = line.split(',')\n",
    "        id_ = int(parts[0])\n",
    "        \n",
    "        if id_ in user_id_mapping:\n",
    "            ids.append(r)\n",
    "            docs.append(parts[1])\n",
    "            r += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.2 s, sys: 791 ms, total: 49 s\n",
      "Wall time: 56.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = list(map(preprocess,docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231002it [00:00, 601371.58it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('paperid_text.csv', 'w') as fout:\n",
    "    fout.write('paper_id;processed_docs\\n')\n",
    "    for id_,doc in tqdm(enumerate(processed_docs)):\n",
    "        fout.write(str(id_)+';'+','.join(doc)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231002it [00:00, 306978.70it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "with open('paperid_text.csv') as fin:\n",
    "    fin.readline()\n",
    "    id_ = 0\n",
    "    for line in tqdm(fin):\n",
    "        parts = line.split(';')\n",
    "        id_in_file = int(parts[0])\n",
    "        if (id_in_file != id_):\n",
    "            print('wrong id', id_, id_in_file)\n",
    "            break\n",
    "        processed_docs.append(parts[1][:-1].split(','))\n",
    "        id_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.55 s, sys: 40.1 ms, total: 3.59 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=1000, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 s, sys: 94.2 ms, total: 2.62 s\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 16.9 s, total: 1min 44s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.055*queri + 0.032*search + 0.031*process + 0.031*rank + 0.030*constraint + 0.027*estim + 0.026*graph + 0.025*data + 0.024*entiti + 0.024*function\n",
      "Topic 1: 0.047*learn + 0.046*classif + 0.034*featur + 0.033*cluster + 0.032*social + 0.030*larg + 0.030*scale + 0.030*network + 0.028*effect + 0.027*task\n",
      "Topic 2: 0.036*interact + 0.034*time + 0.034*generat + 0.033*model + 0.031*automat + 0.030*domain + 0.029*user + 0.028*rule + 0.027*represent + 0.026*comput\n",
      "Topic 3: 0.046*mobil + 0.043*onlin + 0.040*context + 0.040*track + 0.037*awar + 0.036*activ + 0.034*person + 0.032*privaci + 0.027*group + 0.025*strategi\n",
      "Topic 4: 0.066*inform + 0.059*retriev + 0.047*workshop + 0.047*proceed + 0.044*intern + 0.040*confer + 0.028*technolog + 0.027*answer + 0.026*extract + 0.026*view\n",
      "Topic 5: 0.055*semant + 0.051*base + 0.045*content + 0.044*studi + 0.041*discoveri + 0.041*case + 0.039*measur + 0.036*probabilist + 0.035*agent + 0.028*topic\n",
      "Topic 6: 0.070*recommend + 0.039*program + 0.034*collabor + 0.033*servic + 0.032*orient + 0.032*visual + 0.032*explor + 0.028*filter + 0.026*overview + 0.022*develop\n",
      "Topic 7: 0.048*manag + 0.046*data + 0.032*distribut + 0.032*system + 0.031*databas + 0.031*sensor + 0.031*logic + 0.027*network + 0.025*memori + 0.025*cloud\n",
      "Topic 8: 0.044*pattern + 0.044*mine + 0.040*problem + 0.038*tree + 0.030*index + 0.029*approxim + 0.029*spatial + 0.025*high + 0.025*recognit + 0.023*data\n",
      "Topic 9: 0.049*ontolog + 0.037*detect + 0.037*digit + 0.028*parallel + 0.027*architectur + 0.025*librari + 0.025*reason + 0.024*event + 0.024*support + 0.023*map\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    \n",
    "    words = []\n",
    "    for i, w in enumerate(topic.split('\"')):\n",
    "        if i%2 != 0:\n",
    "            words.append(dictionary.get(int(w)))\n",
    "        else:\n",
    "            words.append(w)\n",
    "    words = ''.join(words)\n",
    "    print('Topic {}: {}'.format(idx,words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231002it [00:04, 46437.04it/s]\n"
     ]
    }
   ],
   "source": [
    "BoW_vectors = {}\n",
    "for p,doc in tqdm(enumerate(bow_corpus)):\n",
    "    tokens = [0]*len(dictionary)\n",
    "    for token,val in doc:\n",
    "        tokens[token] = val\n",
    "    BoW_vectors[p] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33302it [05:42, 97.21it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('groupid_paperid.csv') as fin, open('groupid_bow.csv','w') as fout:\n",
    "    fin.readline()\n",
    "    fout.write('group_id;bag_of_words\\n')\n",
    "    #group_ids = []\n",
    "    for line in tqdm(fin):\n",
    "            parts = line[:-1].split(';')\n",
    "            g = int(parts[0])\n",
    "            paper_ids_g = [int(p) for p in parts[-1].split()]\n",
    "        \n",
    "            tokens_g = np.array([0]*len(dictionary))\n",
    "            for p in np.random.permutation(paper_ids_g)[:500]:\n",
    "                tokens_g += BoW_vectors[p]\n",
    "            fout.write(str(g)+';'+str([(i,val) for i,val in enumerate(tokens_g) if val>0])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del BoW_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate topics from group_id_bow file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [01:07<00:00, 490.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 1.74 s, total: 1min 3s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('groupid_bow.csv', error_bad_lines=False, delimiter=';', skipinitialspace=True);\n",
    "a = []\n",
    "for i in tqdm(range(len(df['bag_of_words']))):\n",
    "    b = eval(df['bag_of_words'][i])\n",
    "    a = a + [b]\n",
    "df = pd.DataFrame({\"id\": df['group_id'] , \"bag_of_words\":a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [00:57<00:00, 581.09it/s]\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "\n",
    "for i in tqdm(df['bag_of_words']):\n",
    "    topics = topics + [lda_model_tfidf.get_document_topics(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = pd.DataFrame({\"id\": df['id'] , \"topics\":topics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics.to_csv('../csv/topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.21478636170952467),\n",
       " (1, 0.16209490377891622),\n",
       " (2, 0.061621287828714733),\n",
       " (3, 0.082658704277063372),\n",
       " (4, 0.047637520078682709),\n",
       " (5, 0.051632862813726296),\n",
       " (6, 0.044930853403263436),\n",
       " (7, 0.091755877927359675),\n",
       " (8, 0.19462335394248678),\n",
       " (9, 0.048258274240262203)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_tfidf.get_document_topics(df['bag_of_words'][6543])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
