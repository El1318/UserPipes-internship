{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['randint']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import operator\n",
    "import datetime\n",
    "from random import randint\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import scipy.spatial\n",
    "from pyemd import emd\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling plan (by Behrooz): https://docs.google.com/document/d/1D7zdEhEa01CYflIUXHmwzJrmEyGJOD-AQKOluV19C1o/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changed codes for crawling are in \"../crawl\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCM code with instructions: https://github.com/tsudalab/SHIMR/tree/master/code/lcm53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The line used to extract groups:  lcm CfI -l 1 -u 5 csv/pmr.csv 10 csv/groups.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some functions on user sets and group sets\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def union(a, b):\n",
    "    return list(set(a) | set(b))\n",
    "\n",
    "def add_to_list(a, b):\n",
    "    return list(a + b)\n",
    "\n",
    "def is_inside(small_g, big_g):\n",
    "    if supports_list[small_g] < supports_list[big_g]:\n",
    "        if len(union(users_list[small_g], users_list[big_g])) == supports_list[big_g]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def jaccard_sim(a,b):\n",
    "    set_a = set(a)\n",
    "    set_b = set(b)\n",
    "    return len(set_a & set_b)/len(set_a | set_b)\n",
    "\n",
    "def coverage(seed_group, groups):\n",
    "    G = []\n",
    "    for g in groups:\n",
    "        G = union(G, users_list[g])\n",
    "    return len(intersect(G,users_list[seed_group]))/supports_list[seed_group]\n",
    "\n",
    "def diversity(groups):\n",
    "    G = []\n",
    "    for g in groups:\n",
    "        G += users_list[g]\n",
    "    if len(G) > 0:\n",
    "        return len(set(G))/len(G)\n",
    "    return 0\n",
    "\n",
    "def unique_users(groups):\n",
    "    G = []\n",
    "    for g in groups:\n",
    "        G += users_list[g]\n",
    "    if len(G) > 0:\n",
    "        return len(set(G))\n",
    "    return 0\n",
    "\n",
    "def descr_diversity(groups):\n",
    "    I = []\n",
    "    for g in groups:\n",
    "        I += items_list[g]\n",
    "    if len(I) > 0:\n",
    "        return len(set(I))/len(I)\n",
    "    return 0\n",
    "    \n",
    "\n",
    "def replace(groups, g, new_g):\n",
    "    new_groups = groups.copy()\n",
    "    new_groups.remove(g)\n",
    "    new_groups.append(new_g)\n",
    "    return new_groups\n",
    "\n",
    "\n",
    "def fast_jaccard_sim(a,b):\n",
    "    return len(a & b)/len(a | b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_groups_by_n_items(items_list):\\n    items_count = {}\\n    max_l = 0\\n    for group, items in items_list.items():\\n        l = len(items)\\n        if l in items_count:\\n            items_count[l].append(group)\\n        else:\\n            items_count[l] = [group]\\n            if l>max_l:\\n                max_l = l\\n    \\n    for l in range(100):\\n        if l not in items_count:\\n            items_count[l] = []\\n        \\n    return items_count'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_groups_data(data):\n",
    "    \n",
    "    #Reads LCM output and produces dictionaries with group characteristics. \n",
    "    \n",
    "    users_list = {} \n",
    "    conference_list = {}\n",
    "    items_list = {} \n",
    "    supports_list = {} \n",
    "    items_count = {}\n",
    "    \n",
    "    group_cnt = 1 \n",
    "    num_users = 0\n",
    "\n",
    "    with open(data, 'r') as fin:\n",
    "        while True:\n",
    "            line1 = fin.readline()\n",
    "            if not line1:\n",
    "                break\n",
    "            split_1 = line1.split(' (')\n",
    "            ids = [int(id_) for id_ in split_1[0].split(' ')]\n",
    "            items_list[group_cnt] = [items_ids[id_] for id_ in ids]\n",
    "            conference_list[group_cnt] = []\n",
    "            for i in ids:\n",
    "                if i > 30000:\n",
    "                    conference_list[group_cnt].append(items_ids[i])\n",
    "            items_count[group_cnt] = len(items_list[group_cnt])\n",
    "            supports_list[group_cnt] = int(split_1[1].split(')')[0])\n",
    "\n",
    "\n",
    "            line2 = fin.readline()\n",
    "            users_list[group_cnt] = [int(id_) for id_ in line2[1:-1].split(' ')]\n",
    "            max_ = max(users_list[group_cnt])\n",
    "            if max_>num_users:\n",
    "                num_users = max_\n",
    "\n",
    "            group_cnt += 1\n",
    "            \n",
    "    users_list[0] = range(0,num_users+1)\n",
    "    items_list[0] = []\n",
    "    supports_list[0] = num_users+1\n",
    "    conference_list[0] = []\n",
    "    items_count[0] = 0\n",
    "   \n",
    "    return users_list, items_list, conference_list, supports_list, items_count\n",
    "\n",
    "def get_inv_index(items_list, items_ids):\n",
    "    items_inv_index = {}\n",
    "    \n",
    "    for _, item in items_ids.items():\n",
    "        items_inv_index[item] = []\n",
    "        \n",
    "    for group, items in items_list.items():\n",
    "        for item in items:\n",
    "            items_inv_index[item].append(group)\n",
    "            \n",
    "    return items_inv_index\n",
    "\n",
    "'''def get_groups_by_n_items(items_list):\n",
    "    items_count = {}\n",
    "    max_l = 0\n",
    "    for group, items in items_list.items():\n",
    "        l = len(items)\n",
    "        if l in items_count:\n",
    "            items_count[l].append(group)\n",
    "        else:\n",
    "            items_count[l] = [group]\n",
    "            if l>max_l:\n",
    "                max_l = l\n",
    "    \n",
    "    for l in range(100):\n",
    "        if l not in items_count:\n",
    "            items_count[l] = []\n",
    "        \n",
    "    return items_count'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load groups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 51.2 ms, total: 1.11 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "items_ids = {}\n",
    "\n",
    "with open('csv/conference_mapping.csv', 'r') as mapping:\n",
    "    mapping.readline()\n",
    "    for line in mapping:\n",
    "        split_ = line.split(',')\n",
    "        items_ids[int(split_[0])] = split_[1][:-1]\n",
    "        \n",
    "with open('csv/topic_mapping.csv', 'r') as mapping:\n",
    "    mapping.readline()\n",
    "    for line in mapping:\n",
    "        split_ = line.split(',')\n",
    "        items_ids[int(split_[0])] = split_[1][:-1]\n",
    "        \n",
    "user_id_mapping = {}\n",
    "\n",
    "with open(\"csv/user_id_mapping.csv\") as f:\n",
    "    for line in f:\n",
    "        parts = line.split(',')\n",
    "        user_id_mapping[int(parts[0])] = int(parts[1])\n",
    "        \n",
    "data = \"csv/groups.csv\"\n",
    "users_list, items_list, conference_list, supports_list, items_count = read_groups_data(data)\n",
    "groups_by_item = get_inv_index(items_list,items_ids) \n",
    "#groups_by_n_items = get_groups_by_n_items(items_list)\n",
    "all_groups = list(range(len(items_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extact rating distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "744515it [00:02, 271896.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 145 ms, total: 2.42 s\n",
      "Wall time: 2.79 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pb_by_user_id = {}\n",
    "with open('csv/publications.csv') as fin:\n",
    "    fin.readline()\n",
    "    \n",
    "    for line in tqdm(fin):\n",
    "        parts = line.split(',')\n",
    "        id_ = int(parts[0])\n",
    "        if id_ in user_id_mapping:\n",
    "            id_ = user_id_mapping[id_]\n",
    "            r = int(parts[3])\n",
    "            conf = parts[2].strip(' ')\n",
    "\n",
    "            if id_ in pb_by_user_id.keys():\n",
    "                if conf in pb_by_user_id[id_].keys():\n",
    "                    pb_by_user_id[id_][conf].append(r)\n",
    "                else: \n",
    "                    pb_by_user_id[id_][conf] = [r]\n",
    "            else:\n",
    "                pb_by_user_id[id_] = {}\n",
    "                pb_by_user_id[id_][conf] = [r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [00:09<00:00, 3419.40it/s]\n"
     ]
    }
   ],
   "source": [
    "distributions_list = {}\n",
    "\n",
    "for g in tqdm(all_groups):\n",
    "    curr_distr = [0]*5\n",
    "    for id_ in users_list[g]:\n",
    "        if len(conference_list[g]) > 0:\n",
    "            for conf in conference_list[g]:\n",
    "                for r in pb_by_user_id[id_][conf]:\n",
    "\n",
    "                    if r <= 2005:\n",
    "                        curr_distr[0] += 1\n",
    "                    elif r <= 2010:\n",
    "                        curr_distr[1] += 1\n",
    "                    elif r <= 2013:\n",
    "                        curr_distr[2] += 1\n",
    "                    elif r <= 2016:\n",
    "                        curr_distr[3] += 1\n",
    "                    else:\n",
    "                        curr_distr[4] += 1  \n",
    "        else:\n",
    "            for conf in pb_by_user_id[id_]:\n",
    "                for r in pb_by_user_id[id_][conf]:\n",
    "\n",
    "                    if r <= 2005:\n",
    "                        curr_distr[0] += 1\n",
    "                    elif r <= 2010:\n",
    "                        curr_distr[1] += 1\n",
    "                    elif r <= 2013:\n",
    "                        curr_distr[2] += 1\n",
    "                    elif r <= 2016:\n",
    "                        curr_distr[3] += 1\n",
    "                    else:\n",
    "                        curr_distr[4] += 1                   \n",
    "                    \n",
    "    num_pb = sum(curr_distr)        \n",
    "    distributions_list[g] = np.array([v/num_pb for v in curr_distr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline: find relevant groups (run only once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is time consuming. Jaccard and EMD similarities matrices and indexes of groups inside each given group are precomputed and saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(users_list[0])\n",
    "inv_index = defaultdict(list)\n",
    "\n",
    "users_by_group_as_list = []\n",
    "for g in all_groups:\n",
    "    users_by_group_as_list.append(set(users_list[g]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [20:58<00:00, 26.45it/s] \n"
     ]
    }
   ],
   "source": [
    "#Specify the threshold of similarity \n",
    "threshold = 0.1\n",
    "\n",
    "for i in tqdm(all_groups[:]):\n",
    "    a = users_list[i]\n",
    "    s = np.array(list( map(lambda g: jaccard_sim(a,g), users_by_group_as_list[i+1:]) ))\n",
    "    groups = np.argwhere(s>threshold).T[0] + i + 1\n",
    "    inv_index[i].extend(groups)\n",
    "    list(map(lambda g: inv_index[g].append(i), groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [00:16<00:00, 2006.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(inv_index.keys()):\n",
    "    inv_index[key] = list(set(inv_index[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [02:23<00:00, 232.33it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('csv/inverted_index_jaccard.csv', 'w') as fin:\n",
    "    keys = sorted(list(inv_index.keys()))\n",
    "    for key in tqdm(keys):\n",
    "        line = [str(g) for g in inv_index[key]]\n",
    "        print(key, ' '.join(line), sep = ':', end='\\n', file=fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inv_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groups inside "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(all_groups[:]):\n",
    "    s = np.array(list( map(lambda g: is_inside(g,i), all_groups[i+1:]) ))\n",
    "    groups = np.argwhere(s>0).T[0] + i + 1\n",
    "    inside_index[i].extend(groups)\n",
    "    list(map(lambda g: inside_index[g].append(i), groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(inside_index.keys()):\n",
    "    inside_index[key] = list(set(inside_index[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('csv/inverted_index_inside.csv', 'w') as fin:\n",
    "    keys = sorted(list(inside_index.keys()))\n",
    "    for key in tqdm(keys):\n",
    "        line = [str(g) for g in inside_index[key]]\n",
    "        print(key, ' '.join(line), sep = ':', end='\\n', file=fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del inside_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emd_inv_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distr_by_group_as_list = []\n",
    "for g in all_groups:\n",
    "    distr_by_group_as_list.append(distributions_list[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_matrix = 3*np.array([[0,0.25,0.5,0.75,1], \n",
    "                            [0.25,0,0.25,0.5,0.75], \n",
    "                            [0.5,0.25,0,0.25,0.5], \n",
    "                            [0.75,0.5,0.25,0,0.25], \n",
    "                            [1,0.75,0.5,0.25,0]]).astype('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [9:08:08<00:00,  1.01it/s]   \n"
     ]
    }
   ],
   "source": [
    "#Specify the threshold of distance\n",
    "threshold = 0.05\n",
    "\n",
    "for i in tqdm(all_groups[:]):\n",
    "    a = distributions_list[i]\n",
    "    s = np.array(list( map(lambda g: emd(a,g,distance_matrix), distr_by_group_as_list[i+1:]) ))\n",
    "    groups = np.argwhere(s<threshold).T[0] + i + 1\n",
    "    emd_inv_index[i].extend(groups)\n",
    "    list(map(lambda g: emd_inv_index[g].append(i), groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [00:07<00:00, 4405.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(emd_inv_index.keys()):\n",
    "    emd_inv_index[key] = list(set(emd_inv_index[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [01:45<00:00, 315.49it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('csv/inverted_index_emd.csv', 'w') as fin:\n",
    "    keys = sorted(list(emd_inv_index.keys()))\n",
    "    for key in tqdm(keys):\n",
    "        line = [str(g) for g in emd_inv_index[key]]\n",
    "        print(key, ' '.join(line), sep = ':', end='\\n', file=fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline: Topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics for all the groups precomputed in processing.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If already precomputed: load relevant groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_matrix = 3*np.array([[0,0.25,0.5,0.75,1], \n",
    "                            [0.25,0,0.25,0.5,0.75], \n",
    "                            [0.5,0.25,0,0.25,0.5], \n",
    "                            [0.75,0.5,0.25,0,0.25], \n",
    "                            [1,0.75,0.5,0.25,0]]).astype('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33302it [00:00, 48289.18it/s] \n",
      "33302it [00:08, 3833.75it/s]\n",
      "33302it [00:06, 4898.63it/s]\n"
     ]
    }
   ],
   "source": [
    "inside_index = {}\n",
    "\n",
    "with open('csv/inverted_index_inside.csv') as fin:\n",
    "    for line in tqdm(fin):\n",
    "        parts = line.split(':')\n",
    "        group_id = int(parts[0])\n",
    "        similar_groups = [int(g) for g in parts[1].split()]\n",
    "        inside_index[group_id] = similar_groups\n",
    "        \n",
    "inverted_index_jaccard = {}\n",
    "\n",
    "with open('csv/inverted_index_jaccard.csv') as fin:\n",
    "    for line in tqdm(fin):\n",
    "        parts = line.split(':')\n",
    "        group_id = int(parts[0])\n",
    "        similar_groups = [int(g) for g in parts[1].split()]\n",
    "        inverted_index_jaccard[group_id] = similar_groups#list(set(similar_groups) - set(inside_index[group_id]))\n",
    "        \n",
    "inverted_index_emd = {}\n",
    "\n",
    "with open('csv/inverted_index_emd.csv') as fin:\n",
    "    for line in tqdm(fin):\n",
    "        parts = line.split(':')\n",
    "        group_id = int(parts[0])\n",
    "        similar_groups = [int(g) for g in parts[1].split()]\n",
    "        inverted_index_emd[group_id] = similar_groups#list(set(similar_groups) - set(inside_index[group_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33302it [00:03, 11083.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#Specify the number of topics (as in topic extraction)\n",
    "num_topics = 10\n",
    "topics_list = []\n",
    "\n",
    "with open('csv/topics.csv') as fin:\n",
    "    fin.readline()\n",
    "    for line in tqdm(fin):\n",
    "        parts = line.split('\"')\n",
    "        group_id = int(parts[0][:-1])\n",
    "        raw_topics = eval(parts[1])\n",
    "        topics = [0.]*num_topics\n",
    "        def set_topic_val_by_ind(topic):\n",
    "            val = topic[1]\n",
    "            topics[topic[0]] = val*int(val>0.2)\n",
    "        list(map(set_topic_val_by_ind, raw_topics))\n",
    "        topics = np.array(topics)\n",
    "        topics_list.append(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getNext() relevant groups\n",
    "\n",
    "def get_similar_within(groups_space, seed_group):\n",
    "    groups = groups_space.copy()\n",
    "    similar_groups = inside_index[seed_group]\n",
    "    groups = intersect(groups, similar_groups)\n",
    "\n",
    "    return groups\n",
    "\n",
    "def get_similar_around(groups_space, seed_group):\n",
    "    groups = groups_space.copy()\n",
    "    similar_groups = inverted_index_jaccard[seed_group]\n",
    "    groups = intersect(groups, similar_groups)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def get_similar_emd(groups_space, seed_group):\n",
    "    groups = groups_space.copy()\n",
    "    similar_groups = inverted_index_emd[seed_group]\n",
    "    groups = intersect(groups, similar_groups)\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify default time limit\n",
    "T = 100\n",
    "\n",
    "def by_example_around(seed_group,k,lowest_sim=0.2,time_limit=T,all_groups_space=all_groups.copy()):\n",
    "    groups_space = all_groups_space[:]\n",
    "    groups = get_similar_around(groups_space, seed_group)\n",
    "   \n",
    "    if (k != -1) & (len(groups) > k):\n",
    "        groups = greedy_max_diversity(groups,k,time_limit)\n",
    "        \n",
    "    return groups\n",
    "\n",
    "def by_example_within(seed_group,k,lowest_sim=0.2,time_limit=T,all_groups_space=all_groups.copy()):\n",
    "    groups_space = all_groups_space[:]\n",
    "    groups = get_similar_within(groups_space, seed_group)\n",
    "    \n",
    "    if (k != -1) & (len(groups) > k):\n",
    "        groups = greedy_max_coverage(groups,seed_group,k,time_limit)\n",
    "        \n",
    "    return groups\n",
    "\n",
    "def by_distribution(seed_group,k,lowest_sim=0.0,time_limit=T,all_groups_space=all_groups.copy()):\n",
    "    groups_space = all_groups_space[:]\n",
    "    groups = get_similar_emd(groups_space, seed_group)\n",
    "\n",
    "    if (k != -1) & (len(groups) > k):\n",
    "        groups = greedy_max_diversity(groups,k,time_limit)\n",
    "        \n",
    "    return groups\n",
    "\n",
    "def by_topic(seed_group, k, time_limit = T, cosine_sim_threshold = 0.1):\n",
    "    u = topics_list[seed_group]\n",
    "    vals = np.array([u.dot(v) for v in topics_list])\n",
    "    groups = list(np.argwhere(vals > cosine_sim_threshold).T[0])\n",
    "    \n",
    "    if seed_group in groups:\n",
    "        groups.remove(seed_group)\n",
    "    \n",
    "    if (k != -1) & (len(groups) > k):\n",
    "        groups = greedy_max_descr_diversity(groups,k,time_limit)\n",
    "        \n",
    "    return groups\n",
    "\n",
    "\n",
    "\n",
    "facets = {\n",
    "          'gender': ['male', 'female'], \n",
    "          'seniority': [ 'starting', 'junior', 'senior', 'highly senior', 'confirmed'],\n",
    "          'productivity': ['active', 'very active', 'productive', 'very productive', 'prolific'],\n",
    "          'publications': ['very few', 'few', 'fair', 'high', 'very high'],\n",
    "          'country' : ['North America', 'UK/Ireland', 'Europe', 'Asia', 'Australia',\n",
    "                        'South America', 'Middle East', 'other country'] \n",
    "        }\n",
    "\n",
    "\n",
    "def by_facet(seed_group, k, facet, groups_space=all_groups, time_limit = T):\n",
    "    groups = []\n",
    "    items = set(items_list[seed_group]) - set(facets[facet])\n",
    "    \n",
    "    for value in facets[facet]:\n",
    "        attributes = union(items, [value])\n",
    "        value_groups = groups_space\n",
    "        for attribute in attributes:\n",
    "            value_groups = intersect(value_groups, groups_by_item[attribute])\n",
    "        value_supports = [(g,supports_list[g]) for g in value_groups]\n",
    "        value_supports = sorted(value_supports, key=operator.itemgetter(1), reverse=True)\n",
    "        if len(value_supports) > 0:\n",
    "            groups.append(value_supports[0][0])\n",
    "            \n",
    "        if (k != -1) & (len(groups) > k):\n",
    "            groups = greedy_max_coverage(groups,seed_group,k,time_limit)\n",
    "        \n",
    "    return groups\n",
    "\n",
    "def by_gender(seed_group, k):\n",
    "    facet = 'gender'\n",
    "    return by_facet(seed_group, k, facet)\n",
    "\n",
    "def by_country(seed_group, k):\n",
    "    facet = 'country'\n",
    "    return by_facet(seed_group, k, facet)\n",
    "\n",
    "def by_seniority(seed_group, k):\n",
    "    facet = 'seniority'\n",
    "    return by_facet(seed_group, k, facet)\n",
    "\n",
    "def by_productivity(seed_group, k):\n",
    "    facet = 'productivity'\n",
    "    return by_facet(seed_group, k, facet)\n",
    "\n",
    "def by_conference(seed_group, k, groups_space=all_groups, time_limit = T):\n",
    "    groups = []\n",
    "    confs = conference_list[seed_group]\n",
    "    if len(confs) > 0:\n",
    "        groups = groups_space[:]\n",
    "    for conf in confs:\n",
    "        groups = intersect(groups, groups_by_item[conf])\n",
    "        \n",
    "    if (k != -1) & (len(groups) > k):\n",
    "            groups = greedy_max_diversity(groups,k,time_limit)\n",
    "        \n",
    "    return groups\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Greedy quality optimization\n",
    "\n",
    "def greedy_max_diversity(records, k, time_limit, stop_visiting_once = False):\n",
    "    current_records = records[:k]       \n",
    "    new_records = records[:k]     \n",
    "    total_time = 0.0            \n",
    "\n",
    "    pointer = k-1\n",
    "    nb_iterations = 1\n",
    "    nb_lookups = 0\n",
    "    \n",
    "    current_diversity = diversity(current_records)\n",
    "    \n",
    "    while total_time < time_limit:\n",
    "        nb_lookups += 1\n",
    "        pointer += 1\n",
    "\n",
    "        begin_time = datetime.datetime.now()\n",
    "        \n",
    "        replacement = randint(0,k-1)\n",
    "        new_records = current_records[:]\n",
    "        new_records.pop(replacement)\n",
    "        \n",
    "        if records[pointer] not in new_records:\n",
    "            new_records.append(records[pointer])\n",
    "            new_diversity = diversity(new_records)\n",
    "        \n",
    "            if new_diversity >= current_diversity:\n",
    "                current_records = new_records[:]\n",
    "                current_diversity = new_diversity\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        duration = (end_time - begin_time).microseconds / 1000.0\n",
    "        total_time += duration\n",
    "\n",
    "        if pointer >= len(records)-1:\n",
    "            if stop_visiting_once == False:\n",
    "                pointer = k-1\n",
    "                nb_iterations += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    return current_records\n",
    "\n",
    "def greedy_max_coverage(records, seed_group, k, time_limit, stop_visiting_once = False):\n",
    "    current_records = records[:k]       \n",
    "    new_records = records[:k]     \n",
    "    total_time = 0.0            \n",
    "\n",
    "    pointer = k-1\n",
    "    nb_iterations = 1\n",
    "    nb_lookups = 0\n",
    "    \n",
    "    current_coverage = coverage(seed_group,current_records)\n",
    "    \n",
    "    while total_time < time_limit:\n",
    "        nb_lookups += 1\n",
    "        pointer += 1\n",
    "\n",
    "        begin_time = datetime.datetime.now()\n",
    "        \n",
    "        replacement = randint(0,k-1)\n",
    "        new_records = current_records[:]\n",
    "        new_records.pop(replacement)\n",
    "        \n",
    "        if records[pointer] not in new_records:\n",
    "            new_records.append(records[pointer])\n",
    "            new_coverage = coverage(seed_group,new_records)\n",
    "\n",
    "            if new_coverage >= current_coverage:\n",
    "                current_records = new_records[:]\n",
    "                current_coverage = new_coverage\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        duration = (end_time - begin_time).microseconds / 1000.0\n",
    "        total_time += duration\n",
    "\n",
    "        if pointer >= len(records)-1:\n",
    "            if stop_visiting_once == False:\n",
    "                pointer = k-1\n",
    "                nb_iterations += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    return current_records\n",
    "\n",
    "\n",
    "def greedy_max_descr_diversity(records, k, time_limit, stop_visiting_once = False):\n",
    "    current_records = records[:k]       \n",
    "    new_records = records[:k]     \n",
    "    total_time = 0.0            \n",
    "\n",
    "    pointer = k-1\n",
    "    nb_iterations = 1\n",
    "    nb_lookups = 0\n",
    "    \n",
    "    current_diversity = descr_diversity(current_records)\n",
    "    \n",
    "    while total_time < time_limit:\n",
    "        nb_lookups += 1\n",
    "        pointer += 1\n",
    "\n",
    "        begin_time = datetime.datetime.now()\n",
    "        \n",
    "        replacement = randint(0,k-1)\n",
    "        new_records = current_records[:]\n",
    "        new_records.pop(replacement)\n",
    "        \n",
    "        if records[pointer] not in new_records:\n",
    "            new_records.append(records[pointer])\n",
    "            new_diversity = descr_diversity(new_records)\n",
    "        \n",
    "            if new_diversity >= current_diversity:\n",
    "                current_records = new_records[:]\n",
    "                current_diversity = new_diversity\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        duration = (end_time - begin_time).microseconds / 1000.0\n",
    "        total_time += duration\n",
    "\n",
    "        if pointer >= len(records)-1:\n",
    "            if stop_visiting_once == False:\n",
    "                pointer = k-1\n",
    "                nb_iterations += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    return current_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_groups(current_records):\n",
    "    k = len(current_records)\n",
    "    if k > 0:\n",
    "        for i in range(k):\n",
    "            print(str(i+1)+\". G\"+str(current_records[i])+\": \"+str(items_list[current_records[i]])\n",
    "                  +\"(\"+str(supports_list[current_records[i]])+\" members)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Explore(groups_space, exploration_type, **kwargs):\n",
    "    if exploration_type == 'by-facet':  \n",
    "        facet = kwargs.get('facet')\n",
    "        seed_group = kwargs.get('seed_group', 5)\n",
    "        k = kwargs.get('groups_num', 5)\n",
    "        time_limit = kwargs.get('time_limit', 1000)\n",
    "        \n",
    "        print('seed: '+str(items_list[seed_group]))\n",
    "        groups = by_facet(seed_group,k,facet,groups_space.copy(),time_limit)\n",
    "\n",
    "    elif exploration_type == 'by-example-around':\n",
    "        seed_group = kwargs['seed_group']\n",
    "        k = kwargs.get('groups_num', 5)\n",
    "        time_limit = kwargs.get('time_limit', 1000)\n",
    "        lowest_acceptable_similarity = kwargs.get('lowest_acceptable_similarity', 0.2)\n",
    "        \n",
    "        print('seed: '+str(items_list[seed_group]))\n",
    "        groups = by_example_around(seed_group,k,lowest_acceptable_similarity,time_limit,groups_space.copy())\n",
    "        \n",
    "    elif exploration_type == 'by-example-within':    \n",
    "        seed_group = kwargs['seed_group']\n",
    "        k = kwargs.get('groups_num', 5)\n",
    "        time_limit = kwargs.get('time_limit', 1000)\n",
    "        lowest_acceptable_similarity = kwargs.get('lowest_acceptable_similarity', 0.2)\n",
    "        \n",
    "        print('seed: '+str(items_list[seed_group]))\n",
    "        groups = by_example_within(seed_group,k,lowest_acceptable_similarity,time_limit,groups_space.copy())\n",
    "        \n",
    "    elif exploration_type == 'by-distribution': \n",
    "        seed_group = kwargs['seed_group']\n",
    "        k = kwargs.get('groups_num', 5)\n",
    "        time_limit = kwargs.get('time_limit', 1000)\n",
    "        lowest_acceptable_similarity = kwargs.get('lowest_acceptable_similarity', 0.2)\n",
    "        \n",
    "        print('seed: '+str(items_list[seed_group]))\n",
    "        groups = by_distribution(seed_group,k,lowest_acceptable_similarity,time_limit,groups_space.copy())  \n",
    "        \n",
    "    elif exploration_type == 'by-topic': \n",
    "        seed_group = kwargs['seed_group']\n",
    "        k = kwargs.get('groups_num', 5)\n",
    "        \n",
    "        print('seed: '+str(items_list[seed_group]))\n",
    "        groups = by_topic(seed_group,k)\n",
    "      \n",
    "    elif exploration_type == 'by-conference':  \n",
    "        seed_group = kwargs['seed_group']\n",
    "        k = kwargs.get('groups_num', 5)\n",
    "        time_limit = kwargs.get('time_limit', 1000)\n",
    "        \n",
    "        print('seed: '+str(items_list[seed_group]))\n",
    "        groups = by_conference(seed_group,k,groups_space.copy(),time_limit)\n",
    "        \n",
    "    else:\n",
    "        print('Exploration type not recognised!')\n",
    "        return -1\n",
    "    \n",
    "    print_groups(groups)\n",
    "    print(\"- diversity: \" + str(diversity(groups)) + \" (1.0 being the most diverse)\") \n",
    "    print('- coverage: ', coverage(seed_group,groups))\n",
    "                \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['CIKM', 'PVLDB']\n",
      "1. G22: ['CIKM', 'PVLDB', 'male'](113 members)\n",
      "2. G12901: ['female', 'CIKM', 'PVLDB'](22 members)\n",
      "3. G1627: ['Lecture Notes in Computer Science', 'CIKM', 'ICDE', 'PVLDB', 'male'](27 members)\n",
      "4. G11868: ['ICDM', 'IEEE Trans. Knowl. Data Eng.', 'CIKM', 'PVLDB', 'CoRR'](21 members)\n",
      "5. G11869: ['ICDM', 'IEEE Trans. Knowl. Data Eng.', 'CIKM', 'PVLDB', 'male'](21 members)\n",
      "- diversity: 0.6617647058823529 (1.0 being the most diverse)\n",
      "- coverage:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[22, 12901, 1627, 11868, 11869]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore(all_groups, 'by-example-within', seed_group = 20, groups_num = 5, time_limit=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['CIKM', 'PVLDB']\n",
      "1. G39: ['SIGMOD Conference', 'PVLDB', 'male', 'CoRR'](192 members)\n",
      "2. G16: ['CIKM'](500 members)\n",
      "3. G25250: ['IEEE Data Eng. Bull.', 'SIGMOD Conference', 'CIKM', 'ICDE', 'PVLDB'](27 members)\n",
      "4. G15651: ['VLDB J.', 'IEEE Trans. Knowl. Data Eng.', 'prolific', 'CIKM', 'PVLDB'](27 members)\n",
      "5. G5203: ['EDBT', 'SIGMOD Conference', 'CIKM', 'PVLDB', 'CoRR'](27 members)\n",
      "- diversity: 0.8240620957309185 (1.0 being the most diverse)\n",
      "- coverage:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[39, 16, 25250, 15651, 5203]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore(all_groups, 'by-example-around', seed_group = 20, groups_num = 5, time_limit=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['CIKM', 'PVLDB']\n",
      "1. G33293: ['GRADES'](10 members)\n",
      "2. G9495: ['BigData', 'EDBT', 'very productive', 'ICDE', 'PVLDB'](10 members)\n",
      "3. G13322: ['female', 'highly senior', 'Encyclopedia of Database Systems (2nd ed.)'](18 members)\n",
      "4. G13610: ['productive', 'WWW (Companion Volume)', 'North America'](10 members)\n",
      "5. G14392: ['senior', 'IEEE Trans. Knowl. Data Eng.', 'SIGMOD Conference', 'CoRR'](15 members)\n",
      "- diversity: 1.0 (1.0 being the most diverse)\n",
      "- coverage:  0.0962962962962963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[33293, 9495, 13322, 13610, 14392]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore(all_groups, 'by-topic', seed_group = 20, groups_num = 5, time_limit=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['CIKM', 'PVLDB']\n",
      "1. G22940: ['Asia', 'EDBT', 'IEEE Trans. Knowl. Data Eng.', 'CIKM'](10 members)\n",
      "2. G8113: ['highly senior', 'KDD', 'North America', 'SIGMOD Conference', 'male'](10 members)\n",
      "3. G32718: ['Australia', 'Lecture Notes in Computer Science', 'confirmed'](10 members)\n",
      "4. G4582: ['KDD', 'Europe'](38 members)\n",
      "5. G22039: ['Inf. Sci.', 'SIGIR', 'CIKM'](15 members)\n",
      "- diversity: 1.0 (1.0 being the most diverse)\n",
      "- coverage:  0.17037037037037037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[22940, 8113, 32718, 4582, 22039]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore(all_groups, 'by-distribution', seed_group = 20, groups_num = 5, time_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['CIKM', 'PVLDB']\n",
      "1. G22: ['CIKM', 'PVLDB', 'male'](113 members)\n",
      "2. G12901: ['female', 'CIKM', 'PVLDB'](22 members)\n",
      "- diversity: 1.0 (1.0 being the most diverse)\n",
      "- coverage:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[22, 12901]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore(all_groups, 'by-facet',  facet = 'gender', seed_group = 20, groups_num = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['ICDE', 'male']\n",
      "1. G25936: ['IEEE Data Eng. Bull.', 'senior', 'ICDE', 'PVLDB', 'CoRR'](10 members)\n",
      "2. G9221: ['BigData', 'very productive', 'North America', 'ICDE'](11 members)\n",
      "3. G1083: ['Europe', 'prolific', 'Encyclopedia of Database Systems (2nd ed.)', 'SIGMOD Conference', 'ICDE'](14 members)\n",
      "4. G32817: ['Australia', 'VLDB J.', 'ICDE', 'PVLDB', 'CoRR'](15 members)\n",
      "5. G1357: ['IEEE Trans. Knowl. Data Eng.', 'North America', 'Encyclopedia of Database Systems (2nd ed.)', 'confirmed', 'ICDE'](15 members)\n",
      "- diversity: 1.0 (1.0 being the most diverse)\n",
      "- coverage:  0.16666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25936, 9221, 1083, 32817, 1357]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore(all_groups, 'by-conference', seed_group = 10, groups_num = 5, lowest_acceptable_similarity = 0.0,\n",
    "        time_limit=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning exploration strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use case: PCs gathering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids = {}\n",
    "\n",
    "with open(\"csv/authors_unique.csv\") as f_authors:\n",
    "    f_authors.readline()\n",
    "    for line in f_authors:\n",
    "        line = line.strip()\n",
    "        parts = line.split(\",\")\n",
    "        author_code = parts[0]\n",
    "        raw_name = parts[1].strip().split(' ')\n",
    "        author_name = raw_name[0] + ' ' + raw_name[-1]\n",
    "        if author_name not in list(author_ids.keys()):\n",
    "            author_ids[author_name] = int(author_code)\n",
    "        #else:\n",
    "            #print(\"Found another \" + author_name)\n",
    "            \n",
    "        if len(raw_name) > 2:\n",
    "            author_name = raw_name[0] + ' ' + raw_name[1] + ' ' + raw_name[2] \n",
    "            if author_name not in author_ids:\n",
    "                author_ids[author_name] = int(author_code)\n",
    "        if len(raw_name) > 3:\n",
    "            author_name = raw_name[0] + ' ' + raw_name[1] + ' ' + raw_name[2] + ' ' + raw_name[3] \n",
    "            if author_name not in author_ids:\n",
    "                author_ids[author_name] = int(author_code)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luis Galarraga not found in authors!\n",
      "Danai Symeonidou not found in authors!\n"
     ]
    }
   ],
   "source": [
    "PC_raw = ['Fei Chiang','Valter Crescenzi','Daniel Deutch','Irini Fundulaki','Luis Galarraga',\n",
    "          'Melanie Herschel','Aidan Hogan','H. V. Jagadish','Yaron Kanza',\n",
    "          'Ioana Manolescu','Beng Chin Ooi','Paolo Papotti','Rachel Pottinger','Simon Razniewski', \n",
    "          'Sudip Roy','Marc Spaniol','Danai Symeonidou','Saravanan Thirumuruganathan','Yannis Velegrakis']\n",
    "\n",
    "                \n",
    "with open(\"csv/WebDB2017PC.csv\", 'w') as PC:\n",
    "    for line in PC_raw:\n",
    "        raw_name = line.split(' ')\n",
    "        name = line\n",
    "        if len(raw_name) <= 2 and name in author_ids:\n",
    "            PC.write(str(author_ids[name])+','+name+'\\n')\n",
    "        else:\n",
    "            name = raw_name[0] + ' ' + raw_name[-1]\n",
    "            if name in author_ids:\n",
    "                PC.write(str(author_ids[name])+','+name+'\\n')\n",
    "            else:\n",
    "                print(name + \" not found in authors!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luis Galarraga not found in authors!\n"
     ]
    }
   ],
   "source": [
    "PC_raw = ['Matthias Boehm', 'Angela Bonifati','Fei Chiang', 'Xu Chu', \n",
    "            'Valter Crescenzi', 'Marina Danilevsky','Luis Galarraga', 'Wolfgang Gatterbauer',\n",
    "            'Wendy H. Wang', 'Yaron Kanza', 'Guoliang Li', 'Jignesh M. Patel', 'Ioana Manolescu',\n",
    "            'Amelie Marian', 'Felix Naumann', 'Kun Qian', 'Theodoros Rekatsinas', \n",
    "            'Sudip Roy', 'Semih Salihoglu', 'Vasilis Vassalos', 'Yannis Velegrakis', 'Jiannan Wang', \n",
    "            'Jun Yang']\n",
    "\n",
    "with open(\"csv/WebDB2018PC.csv\", 'w') as PC:\n",
    "    for line in PC_raw:\n",
    "        raw_name = line.split(' ')\n",
    "        name = line\n",
    "        if len(raw_name) <= 2 and name in author_ids:\n",
    "            PC.write(str(author_ids[name])+','+name+'\\n')\n",
    "        else:\n",
    "            name = raw_name[0] + ' ' + raw_name[-1]\n",
    "            if name in author_ids:\n",
    "                PC.write(str(author_ids[name])+','+name+'\\n')\n",
    "            else:\n",
    "                print(name + \" not found in authors!\")\n",
    "\n",
    "\n",
    "\n",
    "del author_ids\n",
    "del PC_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"csv/WebDB2017PC.csv\") as inp:\n",
    "    PC = []\n",
    "    for line in inp:\n",
    "        split_ = line.split(',')\n",
    "        PC.append(user_id_mapping[int(split_[0])])\n",
    "        \n",
    "with open(\"csv/WebDB2018PC.csv\") as inp:\n",
    "    testPC = []\n",
    "    for line in inp:\n",
    "        split_ = line.split(',')\n",
    "        testPC.append(user_id_mapping[int(split_[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, init_display, targets, k):\n",
    "        self.C = 0.1\n",
    "        self.targets = targets\n",
    "        self.k = k\n",
    "        \n",
    "        self.display_history = []\n",
    "        self.seed_history = []\n",
    "        self.action_history = []\n",
    "        self.reward_history = [0]\n",
    "        self.seen_targets = []\n",
    "        self.seen_groups = []\n",
    "        self.targets_visits = [0]*len(targets)\n",
    "        \n",
    "        self.init_display = init_display\n",
    "        self.display = init_display\n",
    "        self.seed = self.choose_seed()\n",
    "        self.state = self.get_state()\n",
    "        self.actions = [by_example_around, by_example_within, self.undo, by_gender, by_country, \n",
    "                        by_seniority, by_productivity, by_distribution, by_topic, by_conference]\n",
    "        \n",
    "        \n",
    "    def undo(self, seed, k):\n",
    "        new_display = self.display_history[max(-2, -len(self.display_history))]\n",
    "        self.display_history = self.display_history[:max(-2, -len(self.display_history)+1)]\n",
    "        return new_display\n",
    "        \n",
    "    def step(self, action):    \n",
    "        self.seed_history.append(self.seed)\n",
    "        self.action_history.append(action)\n",
    "        self.display_history.append(self.display)\n",
    "        \n",
    "        self.display = self.actions[action](self.seed, self.k)    \n",
    "        self.seed = self.choose_seed() \n",
    "        reward = self.get_reward(action)\n",
    "        \n",
    "        \n",
    "        self.reward_history.append(reward)\n",
    "        self.state = self.get_state()\n",
    "        \n",
    "        if reward > 0:\n",
    "            self.seen_targets = union(self.seen_targets, set(intersect(users_list[self.seed], self.targets)))\n",
    "            self.seen_groups = union(self.seen_groups, [self.seed])\n",
    "\n",
    "        return self.state, reward\n",
    "    \n",
    "    def choose_seed(self):\n",
    "        max_targets = 0\n",
    "        visited_targets = []\n",
    "        new_seed = None\n",
    "        \n",
    "        groups = list(set(self.display) - set(self.seen_groups))\n",
    "        if len(groups) > 0:\n",
    "            new_seed = np.random.choice(groups)\n",
    "            \n",
    "        seed_candidates = []\n",
    "        for g in groups:\n",
    "            targets_in_g = set(intersect(users_list[g], self.targets)) - set(self.seen_targets)\n",
    "            num_targets = len(targets_in_g)\n",
    "            if num_targets > 0: \n",
    "                seed_candidates.append(g)\n",
    "        \n",
    "        if len(seed_candidates) > 0:\n",
    "            new_seed = np.random.choice(seed_candidates)\n",
    "\n",
    "        return new_seed\n",
    "    \n",
    "    def get_reward(self,action):\n",
    "        if self.seed is not None:\n",
    "            targets_in_seed = set(intersect(users_list[self.seed], self.targets))\n",
    "        \n",
    "            if len(targets_in_seed)/len(users_list[self.seed]) >= self.C:\n",
    "                reward = len(intersect(set(users_list[self.seed]) - set(self.seen_targets), self.targets))#/len(users_list[self.seed])\n",
    "\n",
    "                for t in range(len(self.targets)):\n",
    "                    if self.targets[t] in targets_in_seed:\n",
    "                        self.targets_visits[t] += 1\n",
    "\n",
    "                return reward\n",
    "       \n",
    "        return 0\n",
    "       \n",
    "    \n",
    "    def get_state(self):\n",
    "        new_state = [0]*54\n",
    "        \n",
    "        if len(self.display)>0:\n",
    "            new_state[min(int(diversity(self.display)/0.2),4)] = 1 #[0,4]\n",
    "            new_state[5+int(len(self.display)>1)] = 1 #[5,6] \n",
    "            if len(self.seed_history)>0 and self.seed_history[-1] is not None:\n",
    "                new_state[7+min(int(coverage(self.seed_history[-1],self.display)/0.2),4)] #[7,11]\n",
    "            \n",
    "\n",
    "        if self.seed is not None:\n",
    "            new_state[11] = int(supports_list[self.seed] <= 15) \n",
    "            new_state[12] = int((supports_list[self.seed] > 15) and (supports_list[self.seed] <= 50))\n",
    "            new_state[13] = int((supports_list[self.seed] > 50) and (supports_list[self.seed] <= 100))\n",
    "            new_state[14] = int((supports_list[self.seed] > 100) and (supports_list[self.seed] <= 200))\n",
    "            new_state[15] = int((supports_list[self.seed] > 200) and (supports_list[self.seed] <= 500))\n",
    "            new_state[16] = int(supports_list[self.seed] > 500)#[11,16] \n",
    "            \n",
    "            num_it = len(conference_list[self.seed])\n",
    "            new_state[17] = int(num_it == 0)\n",
    "            new_state[18] = int((num_it > 0) and (num_it <= 2))\n",
    "            new_state[19] = int(num_it > 2)\n",
    "            \n",
    "            num_it = len(set(items_list[self.seed]) - set(conference_list[self.seed]))\n",
    "            new_state[20] = int(num_it == 0)\n",
    "            new_state[21] = int((num_it > 0) and (num_it <= 2))\n",
    "            new_state[22] = int(num_it > 2)\n",
    "            \n",
    "            div = len(intersect(users_list[self.seed], self.seen_targets))\n",
    "            new_state[23] = int(div == 0)\n",
    "            new_state[24] = int(div > 0) #[23,24]\n",
    "            \n",
    "            \n",
    "            emds = np.array([emd(distributions_list[self.seed],np.array([1./5,1./5,1./5,1./5,1./5]),distance_matrix),\n",
    "                    emd(distributions_list[self.seed],np.array([1.,0,0,0,0]),distance_matrix),\n",
    "                    emd(distributions_list[self.seed],np.array([0,0,0,0,1.]),distance_matrix)])\n",
    "            \n",
    "            new_state[25+np.argmax(emds)] = 1 #[25,27]\n",
    "            \n",
    "            div = len(self.seen_targets)\n",
    "            new_state[28+min(int(div/2),4)] = 1 #[28,32]\n",
    "        \n",
    "            if len(intersect(facets['gender'], items_list[self.seed]))>0:\n",
    "                new_state[33] = 1\n",
    "            else:\n",
    "                new_state[34] = 1\n",
    "            if len(intersect(facets['country'], items_list[self.seed]))>0:\n",
    "                new_state[35] = 1\n",
    "            else:\n",
    "                new_state[36] = 1\n",
    "            if len(intersect(facets['seniority'], items_list[self.seed]))>0:\n",
    "                new_state[37] = 1\n",
    "            else:\n",
    "                new_state[38] = 1\n",
    "            if len(intersect(facets['productivity'], items_list[self.seed]))>0:\n",
    "                new_state[39] = 1   \n",
    "            else:\n",
    "                new_state[40] = 1\n",
    "\n",
    "\n",
    "        if self.reward_history[-1] > 0:\n",
    "            new_state[41] = 1\n",
    "        else:\n",
    "            new_state[42] = 1\n",
    "               \n",
    "        if len(self.action_history) > 2:\n",
    "            new_state[43+self.action_history[-1]] = 1\n",
    "            \n",
    "        return new_state\n",
    "    \n",
    "    def reset(self):\n",
    "        self.display = self.init_display\n",
    "        self.display_history = [self.display]\n",
    "        self.action_history = []\n",
    "        self.reward_history = [0]\n",
    "        self.seen_targets = []\n",
    "        self.seen_groups = []\n",
    "        self.targets_visits = [0]*len(self.targets)\n",
    "        \n",
    "        self.seed = self.choose_seed()\n",
    "        self.seed_history = [self.seed]\n",
    "        self.state = self.get_state()\n",
    "        \n",
    "        return self.state, 0 \n",
    "    \n",
    "    def random_reset(self):\n",
    "        \n",
    "        self.display = np.random.permutation(all_groups)[:self.k]\n",
    "       \n",
    "        self.display_history = [self.display]\n",
    "        \n",
    "        self.action_history = []\n",
    "        self.reward_history = [0]\n",
    "        self.seen_targets = []\n",
    "        self.seen_groups = []\n",
    "        self.targets_visits = [0]*len(self.targets)\n",
    "        \n",
    "        self.seed = self.choose_seed()\n",
    "        self.seed_history = [self.seed]\n",
    "        self.state = self.get_state()\n",
    "        \n",
    "        return self.state, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, init_weights, eps=0.1, alpha=0.01, gamma=0.5):\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.array(init_weights)\n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
    "        self.freq = np.array([0]*len(self.actions))\n",
    "    \n",
    "    def tiling(self, state, a):\n",
    "        return np.array([0]*(len(state)*a)+list(state)+[0]*(len(state)*(len(self.actions)-a-1)))\n",
    "        \n",
    "        \n",
    "    def get_random_action(self, state):\n",
    "        new_action = np.argmin(self.freq)\n",
    "        self.freq[new_action] += 1\n",
    "        return new_action\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        possible_actions = self.actions\n",
    "        \n",
    "        if sum(state[11:17]) == 0: #if seed is None only undo \n",
    "            return 2\n",
    "\n",
    "        rand = np.random.choice([0, 1], p = [1 - self.eps, self.eps] )\n",
    "        if rand == 1:\n",
    "            return self.get_random_action(state)\n",
    " \n",
    "        max_Q = -np.inf\n",
    "        new_action = np.random.choice(possible_actions)\n",
    "        \n",
    "        possible_new_actions = []\n",
    "        \n",
    "        for a in possible_actions:\n",
    "            state_tiling = self.tiling(state, a)\n",
    "            Q = sum(state_tiling*self.weights)\n",
    "            if Q > max_Q:\n",
    "                max_Q = Q\n",
    "                possible_new_actions = [a]\n",
    "            elif Q == max_Q:\n",
    "                possible_new_actions.append(a)\n",
    "                \n",
    "        new_action = np.random.choice(possible_new_actions)\n",
    "        self.freq[new_action] += 1\n",
    "        \n",
    "        return new_action\n",
    "        \n",
    "    def sarsa_update_weights(self,s,a,r,new_s,new_a):\n",
    "        self.weights = self.weights + self.alpha*(r+self.gamma*sum(self.weights*self.tiling(new_s, new_a))-\n",
    "                                                  sum(self.weights*self.tiling(s, a)))*self.tiling(s,a)\n",
    "        \n",
    "    def sarsa_terminal_update(self,s,a,r):\n",
    "        self.weights = self.weights + self.alpha*(r-sum(self.weights*self.tiling(s, a)))*self.tiling(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_names = {0:'by_example_around', 1:'by_example_within', 2:'undo',\n",
    "                3:'by_gender', 4:'by_country', 5:'by_seniority', 6:'by_productivity',\n",
    "                7:'by_distribution', 8: 'by-topic', 9:'by-conference'}\n",
    "\n",
    "def semi_gradient_sarsa(environment, policy, episodes_num = 100, episode_length = 15, verbose = False):\n",
    "    history = {'weights': [], 'rewards' : [], 'steps': []}\n",
    "    rewards = []\n",
    "    for episode in range(episodes_num):\n",
    "        episode_rewards = []\n",
    "        episode_weights = []\n",
    "        s, r = environment.reset()\n",
    "        a = policy.get_action(s)\n",
    "\n",
    "        episode_weights.append(policy.weights)\n",
    "\n",
    "        for t in range(episode_length):\n",
    "            new_s, r = environment.step(a)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Action: ', action_names[a])\n",
    "                print('Display:')\n",
    "                for i,g in enumerate(environment.display):\n",
    "                    print(i, ': ', items_list[g], len(users_list[g]), len(intersect(users_list[g],environment.targets)))\n",
    "\n",
    "                if environment.seed is None:\n",
    "                    print('Seed: None')\n",
    "                else:\n",
    "                    print('Seed: ', items_list[environment.seed])\n",
    "                    print('size: ', len(users_list[environment.seed]),\n",
    "                          ', targets: ', len(intersect(users_list[environment.seed],environment.targets)))\n",
    "\n",
    "                print('Reward: ', r)\n",
    "            \n",
    "            \n",
    "            episode_rewards.append(r)\n",
    "            if sum(episode_rewards) >= len(environment.targets)/2:\n",
    "                s = new_s\n",
    "                break\n",
    "            new_a = policy.get_action(new_s)\n",
    "            policy.sarsa_update_weights(s,a,r,new_s,new_a)\n",
    "            episode_weights.append(policy.weights)\n",
    "            s = new_s\n",
    "            a = new_a\n",
    "        \n",
    "        policy.sarsa_terminal_update(s,a,r)\n",
    "        episode_weights.append(policy.weights)\n",
    "        episode_rewards.append(r)\n",
    "        \n",
    "        rewards.append(sum(episode_rewards))\n",
    "        history['weights'].append(episode_weights)\n",
    "        history['rewards'].append(episode_rewards)\n",
    "        history['steps'].append(t)\n",
    "        \n",
    "        if verbose:\n",
    "            print(environment.targets_visits)\n",
    "            \n",
    "    return rewards, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_fet = 54\n",
    "num_actions = 10\n",
    "\n",
    "environment = Environment(init_display = [0], targets = PC, k = 5)\n",
    "test_environment = Environment(init_display = [0], targets = testPC, k = 5)\n",
    "\n",
    "new_policy = Policy(init_weights = [0]*(num_fet*num_actions), eps = 1., alpha = 0.002, gamma = 0.5)\n",
    "test_policy = Policy(init_weights = [0]*(num_fet*num_actions), eps = 0., alpha = 0.00, gamma = 0.5)\n",
    "\n",
    "new_rewards = []\n",
    "new_steps = []\n",
    "test_rewards = []\n",
    "test_steps = []\n",
    "\n",
    "new_history = {'weights': [], 'rewards' : [], 'steps': []}\n",
    "test_history = {'weights': [], 'rewards' : [], 'steps': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save multiple learning curves for plotting\n",
    "learning_curves_train = []\n",
    "learning_curves_test = []\n",
    "learned_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curves_number = 10\n",
    "episodes_number = 500\n",
    "\n",
    "for i in tqdm(range(curves_number)):\n",
    "    new_policy = Policy(init_weights = [0]*(num_fet*num_actions))\n",
    "    test_policy = Policy(init_weights = [0]*(num_fet*num_actions))\n",
    "    new_rewards = []\n",
    "    new_steps = []\n",
    "    test_rewards = []\n",
    "    test_steps = []\n",
    "    \n",
    "    for eps in tqdm(10./np.array(range(10,episodes_number+10,1))): \n",
    "        freq = new_policy.freq\n",
    "        test_policy = Policy(init_weights = list(new_policy.weights[:]), eps = eps, alpha = 0.0, gamma = 0.5)\n",
    "        test_policy.freq = freq\n",
    "        rewards, history = semi_gradient_sarsa(test_environment, test_policy, episodes_num = 1, episode_length = 100)\n",
    "        test_rewards += rewards\n",
    "        #test_steps += history['steps']\n",
    "\n",
    "\n",
    "        new_policy = Policy(init_weights = list(new_policy.weights[:]), eps = eps, alpha = 0.002, gamma = 0.5)\n",
    "        new_policy.freq = freq\n",
    "        rewards, history = semi_gradient_sarsa(environment, new_policy, episodes_num = 1, episode_length = 100)\n",
    "        new_rewards += rewards\n",
    "        new_steps += history['steps']\n",
    "        #total_steps += history['steps'][0]\n",
    "\n",
    "        freq = new_policy.freq\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    #plt.plot([np.mean(new_steps[i:i+10]) for i in range(len(new_steps)-10)], label = 'steps')\n",
    "    plt.plot([np.mean(new_steps[i:i+10]) for i in range(len(new_steps)-10)], label = 'train')\n",
    "    plt.plot([np.mean(test_steps[i:i+10]) for i in range(len(new_steps)-10)], label = 'test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    learning_curves_train.append(new_steps)\n",
    "    learning_curves_test.append(test_steps)\n",
    "    learned_weights.append(list(new_policy.weights[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('steps_train.pickle','wb') as f:\n",
    "    pickle.dump(learning_curves_train, f)\n",
    "\n",
    "with open('steps_test.pickle','wb') as f:\n",
    "    pickle.dump(learning_curves_test, f)\n",
    "\n",
    "with open('steps_learned_weights.pickle','wb') as f:\n",
    "    pickle.dump(learned_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['VLDB J.', 'Europe', 'confirmed', 'ICDE', 'male'] 12 0\n",
      "1 :  ['IEEE Data Eng. Bull.', 'highly senior', 'prolific', 'PVLDB'] 12 1\n",
      "2 :  ['SAC', 'confirmed', 'CIKM', 'male'] 10 0\n",
      "3 :  ['very productive', 'North America', 'PVLDB', 'male'] 51 5\n",
      "4 :  ['female', 'confirmed', 'ICDE'] 27 1\n",
      "Seed:  ['female', 'confirmed', 'ICDE']\n",
      "size:  27 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['highly senior', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'ICDE'] 14 0\n",
      "1 :  ['highly senior', 'IEEE Trans. Knowl. Data Eng.', 'Europe', 'ICDE', 'male'] 10 2\n",
      "2 :  ['VLDB J.', 'EDBT', 'North America', 'ICDE', 'male'] 11 0\n",
      "3 :  ['Knowl. Inf. Syst.', 'WSDM', 'prolific', 'ICDE'] 10 0\n",
      "4 :  ['Australia', 'Lecture Notes in Computer Science', 'prolific', 'ICDE', 'CoRR'] 10 0\n",
      "Seed:  ['highly senior', 'IEEE Trans. Knowl. Data Eng.', 'Europe', 'ICDE', 'male']\n",
      "size:  10 , targets:  2\n",
      "Reward:  2\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['very productive', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'ICDE', 'PVLDB'] 11 2\n",
      "1 :  ['Asia', 'very productive', 'IEEE Trans. Knowl. Data Eng.', 'ICDE', 'male'] 12 0\n",
      "2 :  ['IEEE Access', 'IEEE Trans. Knowl. Data Eng.', 'prolific', 'ICDE', 'CoRR'] 12 0\n",
      "3 :  ['EDBT', 'IEEE Trans. Knowl. Data Eng.', 'Europe', 'Encyclopedia of Database Systems (2nd ed.)', 'ICDE'] 13 2\n",
      "4 :  ['Asia', 'IEEE Trans. Knowl. Data Eng.', 'prolific', 'SIGMOD Conference', 'ICDE'] 15 1\n",
      "Seed:  ['EDBT', 'IEEE Trans. Knowl. Data Eng.', 'Europe', 'Encyclopedia of Database Systems (2nd ed.)', 'ICDE']\n",
      "size:  13 , targets:  2\n",
      "Reward:  1\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['ACM TIST', 'SDM', 'ICDM', 'ICDE', 'CoRR'] 10 0\n",
      "1 :  ['IJCAI', 'Europe', 'ICDE', 'male'] 10 0\n",
      "2 :  ['EDBT', 'very productive', 'Europe', 'confirmed', 'CoRR'] 14 1\n",
      "3 :  ['UK/Ireland', 'EDBT', 'CoRR'] 11 0\n",
      "4 :  ['Inf. Syst.', 'prolific', 'Encyclopedia of Database Systems (2nd ed.)', 'ICDE', 'CoRR'] 11 0\n",
      "Seed:  ['EDBT', 'very productive', 'Europe', 'confirmed', 'CoRR']\n",
      "size:  14 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['highly senior', 'EDBT', 'North America', 'ICDE', 'CoRR'] 12 0\n",
      "1 :  ['highly senior', 'EDBT', 'Europe', 'CIKM', 'CoRR'] 10 1\n",
      "2 :  ['starting', 'EDBT', 'male', 'CoRR'] 13 0\n",
      "3 :  ['junior', 'EDBT', 'CoRR'] 25 1\n",
      "4 :  ['UK/Ireland', 'EDBT', 'CoRR'] 11 0\n",
      "Seed:  ['junior', 'EDBT', 'CoRR']\n",
      "size:  25 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['EDBT', 'very productive', 'Europe', 'confirmed', 'CoRR'] 14 1\n",
      "1 :  ['highly senior', 'EDBT', 'IEEE Trans. Knowl. Data Eng.', 'ICDE', 'CoRR'] 15 2\n",
      "2 :  ['junior', 'EDBT', 'PVLDB', 'male', 'CoRR'] 14 0\n",
      "3 :  ['very active', 'EDBT', 'CoRR'] 10 1\n",
      "4 :  ['BigData', 'EDBT', 'confirmed', 'SIGMOD Conference', 'CoRR'] 14 0\n",
      "Seed:  ['very active', 'EDBT', 'CoRR']\n",
      "size:  10 , targets:  1\n",
      "Reward:  1\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['EDBT', 'very productive', 'IEEE Trans. Knowl. Data Eng.', 'Europe', 'CoRR'] 11 2\n",
      "1 :  ['CIDR', 'EDBT', 'very productive', 'SIGMOD Conference', 'CoRR'] 11 0\n",
      "2 :  ['productive', 'highly senior', 'EDBT', 'CoRR'] 10 0\n",
      "3 :  ['BigData', 'highly senior', 'EDBT', 'male', 'CoRR'] 11 0\n",
      "4 :  ['Knowl. Inf. Syst.', 'EDBT', 'CIKM', 'male', 'CoRR'] 11 0\n",
      "Seed:  ['EDBT', 'very productive', 'IEEE Trans. Knowl. Data Eng.', 'Europe', 'CoRR']\n",
      "size:  11 , targets:  2\n",
      "Reward:  1\n",
      "Action:  by-topic\n",
      "Display:\n",
      "0 :  ['productive', 'WWW (Companion Volume)', 'North America'] 10 0\n",
      "1 :  ['VLDB J.', 'very productive'] 46 5\n",
      "2 :  ['Australia', 'World Wide Web', 'IEEE Trans. Knowl. Data Eng.', 'ICDE', 'PVLDB'] 10 0\n",
      "3 :  ['SSWS@ISWC'] 13 0\n",
      "4 :  ['GRADES'] 10 0\n",
      "Seed:  ['VLDB J.', 'very productive']\n",
      "size:  46 , targets:  5\n",
      "Reward:  3\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['VLDB J.', 'highly senior', 'IEEE Trans. Knowl. Data Eng.', 'prolific', 'SIGMOD Conference'] 13 2\n",
      "1 :  ['VLDB J.', 'highly senior', 'very productive', 'PVLDB', 'male'] 10 1\n",
      "2 :  ['junior', 'VLDB J.', 'SIGMOD Conference', 'ICDE', 'male'] 12 0\n",
      "3 :  ['starting', 'VLDB J.', 'PVLDB', 'male'] 12 0\n",
      "4 :  ['VLDB J.', 'confirmed', 'SIGMOD Conference', 'male'] 34 0\n",
      "Seed:  ['VLDB J.', 'highly senior', 'very productive', 'PVLDB', 'male']\n",
      "size:  10 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by_example_around\n",
      "Display:\n",
      "0 :  ['VLDB J.', 'highly senior', 'North America', 'PVLDB', 'male'] 15 0\n",
      "1 :  ['VLDB J.', 'highly senior', 'EDBT', 'SIGMOD Conference'] 14 2\n",
      "2 :  ['VLDB J.', 'very productive', 'ICDE', 'male', 'CoRR'] 30 1\n",
      "3 :  ['highly senior', 'very productive', 'North America', 'Encyclopedia of Database Systems (2nd ed.)'] 13 0\n",
      "4 :  ['highly senior', 'EDBT', 'very productive', 'male', 'CoRR'] 14 1\n",
      "Seed:  ['VLDB J.', 'highly senior', 'EDBT', 'SIGMOD Conference']\n",
      "size:  14 , targets:  2\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['VLDB J.', 'EDBT', 'WWW (Companion Volume)', 'PVLDB', 'SIGMOD Conference'] 10 3\n",
      "1 :  ['VLDB J.', 'EDBT', 'Europe', 'confirmed', 'SIGMOD Conference'] 11 1\n",
      "2 :  ['VLDB J.', 'EDBT', 'North America', 'SIGMOD Conference', 'male'] 13 0\n",
      "3 :  ['VLDB J.', 'BigData', 'EDBT', 'SIGMOD Conference', 'male'] 13 0\n",
      "4 :  ['VLDB J.', 'highly senior', 'EDBT', 'SIGMOD Conference', 'CoRR'] 11 1\n",
      "Seed:  ['VLDB J.', 'EDBT', 'Europe', 'confirmed', 'SIGMOD Conference']\n",
      "size:  11 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by_gender\n",
      "Display:\n",
      "Seed: None\n",
      "Reward:  0\n",
      "Action:  undo\n",
      "Display:\n",
      "0 :  ['VLDB J.', 'EDBT', 'WWW (Companion Volume)', 'PVLDB', 'SIGMOD Conference'] 10 3\n",
      "1 :  ['VLDB J.', 'EDBT', 'Europe', 'confirmed', 'SIGMOD Conference'] 11 1\n",
      "2 :  ['VLDB J.', 'EDBT', 'North America', 'SIGMOD Conference', 'male'] 13 0\n",
      "3 :  ['VLDB J.', 'BigData', 'EDBT', 'SIGMOD Conference', 'male'] 13 0\n",
      "4 :  ['VLDB J.', 'highly senior', 'EDBT', 'SIGMOD Conference', 'CoRR'] 11 1\n",
      "Seed:  ['VLDB J.', 'EDBT', 'North America', 'SIGMOD Conference', 'male']\n",
      "size:  13 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['ICDCS', 'Asia', 'male'] 13 0\n",
      "1 :  ['very productive', 'IJCAI', 'North America', 'confirmed', 'male'] 10 0\n",
      "2 :  ['Inf. Syst.', 'confirmed', 'SIGMOD Conference', 'ICDE'] 11 0\n",
      "3 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'SIGMOD Conference'] 15 1\n",
      "4 :  ['IJCAI', 'IEEE Trans. Knowl. Data Eng.', 'prolific', 'North America', 'ICDE'] 11 0\n",
      "Seed:  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'SIGMOD Conference']\n",
      "size:  15 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['CIDR', 'VLDB J.', 'IEEE Trans. Knowl. Data Eng.', 'CoRR', 'SIGMOD Conference'] 12 1\n",
      "1 :  ['CIDR', 'VLDB J.', 'IEEE Trans. Knowl. Data Eng.', 'PVLDB', 'SIGMOD Conference'] 12 2\n",
      "2 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'SIGMOD Conference', 'CIKM', 'PVLDB'] 10 2\n",
      "3 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'SIGMOD Conference', 'male'] 13 1\n",
      "4 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'SIGMOD Conference', 'ICDE'] 13 1\n",
      "Seed:  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'SIGMOD Conference', 'ICDE']\n",
      "size:  13 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'SIGMOD Conference', 'ICDE'] 13 1\n",
      "1 :  ['CIDR', 'EDBT', 'IEEE Trans. Knowl. Data Eng.', 'SIGMOD Conference', 'ICDE'] 14 2\n",
      "2 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'SIGMOD Conference', 'ICDE', 'CoRR'] 21 2\n",
      "3 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'SIGMOD Conference', 'ICDE', 'male'] 19 2\n",
      "4 :  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'SIGMOD Conference', 'ICDE', 'PVLDB'] 22 3\n",
      "Seed:  ['CIDR', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'SIGMOD Conference', 'ICDE']\n",
      "size:  13 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['productive', 'Europe', 'PVLDB', 'male'] 19 0\n",
      "1 :  ['very productive', 'IJCAI', 'North America', 'confirmed', 'male'] 10 0\n",
      "2 :  ['ICDCS', 'IEEE Trans. Knowl. Data Eng.', 'CIKM'] 15 0\n",
      "3 :  ['TKDD', 'North America', 'SIGMOD Conference'] 13 0\n",
      "4 :  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'prolific', 'male'] 13 1\n",
      "Seed:  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'prolific', 'male']\n",
      "size:  13 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'prolific', 'SIGMOD Conference', 'male'] 12 1\n",
      "1 :  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'ICDE', 'PVLDB', 'CoRR'] 12 1\n",
      "2 :  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'SIGMOD Conference', 'male', 'CoRR'] 11 1\n",
      "3 :  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'SIGMOD Conference', 'CIKM', 'ICDE'] 10 1\n",
      "4 :  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'prolific', 'male', 'CoRR'] 11 1\n",
      "Seed:  ['IEEE Data Eng. Bull.', 'Lecture Notes in Computer Science', 'SIGMOD Conference', 'male', 'CoRR']\n",
      "size:  11 , targets:  1\n",
      "Reward:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  by-topic\n",
      "Display:\n",
      "0 :  ['T. Large-Scale Data- and Knowledge-Centered Systems', 'female'] 10 0\n",
      "1 :  ['GLOBECOM'] 25 0\n",
      "2 :  ['CIDR', 'junior', 'North America'] 11 1\n",
      "3 :  ['IEEE Data Eng. Bull.', 'productive'] 17 0\n",
      "4 :  ['ICDCS', 'prolific', 'male'] 40 0\n",
      "Seed:  ['CIDR', 'junior', 'North America']\n",
      "size:  11 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['CIDR', 'very productive', 'SIGMOD Conference', 'male', 'CoRR'] 34 3\n",
      "1 :  ['CIDR', 'female', 'SIGMOD Conference', 'ICDE'] 10 1\n",
      "2 :  ['CIDR', 'very active'] 16 1\n",
      "3 :  ['CIDR', 'productive', 'male'] 18 1\n",
      "4 :  ['CIDR', 'prolific', 'male', 'CoRR'] 36 1\n",
      "Seed:  ['CIDR', 'very active']\n",
      "size:  16 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['CIDR', 'highly senior', 'prolific', 'CoRR', 'PVLDB'] 15 1\n",
      "1 :  ['CIDR', 'highly senior', 'very productive', 'PVLDB', 'male'] 11 0\n",
      "2 :  ['CIDR', 'very active', 'North America'] 11 1\n",
      "3 :  ['starting', 'CIDR', 'CoRR'] 10 1\n",
      "4 :  ['CIDR', 'North America', 'confirmed', 'CoRR'] 28 1\n",
      "Seed:  ['starting', 'CIDR', 'CoRR']\n",
      "size:  10 , targets:  1\n",
      "Reward:  1\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['CIDR', 'Europe', 'CoRR'] 28 2\n",
      "1 :  ['CIDR', 'KDD', 'confirmed', 'SIGMOD Conference', 'CoRR'] 10 0\n",
      "2 :  ['CIDR', 'highly senior', 'North America', 'CoRR'] 22 0\n",
      "3 :  ['CIDR', 'productive', 'CoRR', 'male'] 12 1\n",
      "4 :  ['CIDR', 'very productive', 'North America', 'ICDE', 'CoRR'] 12 2\n",
      "Seed:  ['CIDR', 'very productive', 'North America', 'ICDE', 'CoRR']\n",
      "size:  12 , targets:  2\n",
      "Reward:  2\n",
      "Action:  by-topic\n",
      "Display:\n",
      "0 :  ['NII Shonan Meet. Rep.'] 12 0\n",
      "1 :  ['e-Science'] 10 0\n",
      "2 :  ['GRADES'] 10 0\n",
      "3 :  ['ADMS/IMDM@VLDB'] 10 0\n",
      "4 :  ['confirmed', 'ICDE'] 154 3\n",
      "Seed:  ['confirmed', 'ICDE']\n",
      "size:  154 , targets:  3\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['productive', 'EDBT', 'ICDE', 'male'] 10 0\n",
      "1 :  ['senior', 'very productive', 'ICDE', 'PVLDB', 'male'] 13 0\n",
      "2 :  ['WWW', 'Europe', 'ICDE', 'male', 'CoRR'] 10 0\n",
      "3 :  ['Asia', 'VLDB J.', 'Lecture Notes in Computer Science', 'male', 'ICDE'] 11 1\n",
      "4 :  ['Knowl. Inf. Syst.', 'North America', 'CIKM', 'ICDE', 'CoRR'] 11 0\n",
      "Seed:  ['Asia', 'VLDB J.', 'Lecture Notes in Computer Science', 'male', 'ICDE']\n",
      "size:  11 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['productive', 'WWW', 'CIKM', 'male'] 10 0\n",
      "1 :  ['KDD', 'IJCAI', 'AAAI', 'SIGMOD Conference', 'CoRR'] 10 0\n",
      "2 :  ['EVIA@NTCIR', 'SIGIR', 'CIKM', 'CoRR'] 10 0\n",
      "3 :  ['ACL (1)', 'AAAI', 'prolific', 'SIGIR', 'male'] 10 0\n",
      "4 :  ['VLDB J.', 'EDBT', 'CoRR'] 52 4\n",
      "Seed:  ['VLDB J.', 'EDBT', 'CoRR']\n",
      "size:  52 , targets:  4\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['Future Generation Comp. Syst.', 'confirmed', 'CoRR'] 20 0\n",
      "1 :  ['Middle East', 'SIGIR', 'CoRR'] 17 0\n",
      "2 :  ['CIDR', 'Europe', 'SIGMOD Conference', 'PVLDB', 'CoRR'] 17 2\n",
      "3 :  ['UK/Ireland', 'Lecture Notes in Computer Science', 'prolific', 'CoRR'] 17 0\n",
      "4 :  ['SDM', 'WSDM', 'IJCAI', 'male'] 17 0\n",
      "Seed:  ['UK/Ireland', 'Lecture Notes in Computer Science', 'prolific', 'CoRR']\n",
      "size:  17 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['female', 'Lecture Notes in Computer Science', 'prolific', 'confirmed', 'CoRR'] 15 0\n",
      "1 :  ['Asia', 'Lecture Notes in Computer Science', 'ICDE', 'male', 'CoRR'] 13 1\n",
      "2 :  ['Lecture Notes in Computer Science', 'North America', 'CIKM', 'male', 'CoRR'] 11 0\n",
      "3 :  ['Lecture Notes in Computer Science', 'Europe', 'Encyclopedia of Database Systems (2nd ed.)', 'male', 'CoRR'] 24 1\n",
      "4 :  ['Australia', 'Lecture Notes in Computer Science', 'IEEE Trans. Knowl. Data Eng.', 'PVLDB', 'CoRR'] 10 0\n",
      "Seed:  ['Asia', 'Lecture Notes in Computer Science', 'ICDE', 'male', 'CoRR']\n",
      "size:  13 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['Australia', 'Lecture Notes in Computer Science', 'ICDE', 'CoRR'] 12 0\n",
      "1 :  ['Lecture Notes in Computer Science', 'IEEE Trans. Knowl. Data Eng.', 'North America', 'ICDE', 'CoRR'] 12 0\n",
      "2 :  ['Lecture Notes in Computer Science', 'Europe', 'ICDE', 'CoRR'] 16 1\n",
      "3 :  ['Asia', 'Lecture Notes in Computer Science', 'ICDE', 'CoRR'] 14 1\n",
      "4 :  ['Lecture Notes in Computer Science', 'North America', 'confirmed', 'ICDE', 'CoRR'] 10 0\n",
      "Seed:  ['Asia', 'Lecture Notes in Computer Science', 'ICDE', 'CoRR']\n",
      "size:  14 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['Expert Syst. Appl.', 'SAC', 'CoRR'] 13 0\n",
      "1 :  ['very productive', 'Lecture Notes in Computer Science', 'SIGMOD Conference', 'male', 'CoRR'] 12 0\n",
      "2 :  ['productive', 'CIKM', 'ICDE', 'male'] 12 0\n",
      "3 :  ['VLDB J.', 'highly senior', 'EDBT', 'PVLDB', 'male'] 12 2\n",
      "4 :  ['Australia', 'IEEE Trans. Knowl. Data Eng.', 'ICDE', 'PVLDB', 'CoRR'] 12 0\n",
      "Seed:  ['Expert Syst. Appl.', 'SAC', 'CoRR']\n",
      "size:  13 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-topic\n",
      "Display:\n",
      "Seed: None\n",
      "Reward:  0\n",
      "Action:  undo\n",
      "Display:\n",
      "0 :  ['Expert Syst. Appl.', 'SAC', 'CoRR'] 13 0\n",
      "1 :  ['very productive', 'Lecture Notes in Computer Science', 'SIGMOD Conference', 'male', 'CoRR'] 12 0\n",
      "2 :  ['productive', 'CIKM', 'ICDE', 'male'] 12 0\n",
      "3 :  ['VLDB J.', 'highly senior', 'EDBT', 'PVLDB', 'male'] 12 2\n",
      "4 :  ['Australia', 'IEEE Trans. Knowl. Data Eng.', 'ICDE', 'PVLDB', 'CoRR'] 12 0\n",
      "Seed:  ['very productive', 'Lecture Notes in Computer Science', 'SIGMOD Conference', 'male', 'CoRR']\n",
      "size:  12 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['CHI', 'ICWSM', 'CoRR'] 17 0\n",
      "1 :  ['VLDB J.', 'very productive', 'IEEE Trans. Knowl. Data Eng.', 'CoRR'] 17 2\n",
      "2 :  ['highly senior', 'KDD', 'North America', 'PVLDB'] 17 0\n",
      "3 :  ['very productive', 'AAAI', 'SIGIR'] 16 0\n",
      "4 :  ['ACM TIST', 'Inf. Sci.', 'male'] 16 0\n",
      "Seed:  ['highly senior', 'KDD', 'North America', 'PVLDB']\n",
      "size:  17 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['KDD', 'very productive', 'North America', 'confirmed', 'CoRR'] 10 0\n",
      "1 :  ['Asia', 'KDD', 'WWW (Companion Volume)', 'male'] 10 0\n",
      "2 :  ['NIPS', 'highly senior', 'KDD', 'CoRR', 'male'] 17 0\n",
      "3 :  ['VLDB J.', 'KDD', 'very productive', 'SIGMOD Conference', 'PVLDB'] 10 0\n",
      "4 :  ['VLDB J.', 'WSDM', 'KDD', 'CoRR', 'male'] 12 0\n",
      "Seed:  ['VLDB J.', 'WSDM', 'KDD', 'CoRR', 'male']\n",
      "size:  12 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['VLDB J.', 'WSDM', 'KDD', 'CoRR', 'male'] 12 0\n",
      "Seed:  ['VLDB J.', 'WSDM', 'KDD', 'CoRR', 'male']\n",
      "size:  12 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['CEUR Workshop Proceedings', 'AAAI', 'CIKM', 'CoRR'] 10 0\n",
      "1 :  ['Future Generation Comp. Syst.', 'Inf. Sci.'] 10 0\n",
      "2 :  ['CEUR Workshop Proceedings', 'highly senior', 'IJCAI', 'CoRR', 'male'] 10 0\n",
      "3 :  ['very productive', 'confirmed', 'PVLDB', 'CoRR'] 43 2\n",
      "4 :  ['EMNLP', 'SIGIR', 'CIKM', 'male', 'CoRR'] 27 0\n",
      "Seed:  ['EMNLP', 'SIGIR', 'CIKM', 'male', 'CoRR']\n",
      "size:  27 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['ECIR', 'EMNLP', 'SIGIR', 'CIKM', 'CoRR'] 10 0\n",
      "1 :  ['COLING', 'EMNLP', 'SIGIR', 'CIKM', 'CoRR'] 10 0\n",
      "2 :  ['Asia', 'EMNLP', 'SIGIR', 'CIKM', 'CoRR'] 14 0\n",
      "3 :  ['EMNLP', 'confirmed', 'SIGIR', 'CIKM', 'CoRR'] 11 0\n",
      "4 :  ['EMNLP', 'IJCAI', 'SIGIR', 'CIKM', 'CoRR'] 17 0\n",
      "Seed:  ['EMNLP', 'confirmed', 'SIGIR', 'CIKM', 'CoRR']\n",
      "size:  11 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['ACL (1)', 'EMNLP', 'SIGIR', 'CIKM', 'CoRR'] 13 0\n",
      "1 :  ['EMNLP', 'highly senior', 'SIGIR', 'CIKM', 'CoRR'] 11 0\n",
      "2 :  ['COLING', 'EMNLP', 'SIGIR', 'CIKM', 'CoRR'] 10 0\n",
      "3 :  ['Asia', 'EMNLP', 'SIGIR', 'CIKM', 'CoRR'] 14 0\n",
      "4 :  ['EMNLP', 'confirmed', 'SIGIR', 'CIKM', 'CoRR'] 11 0\n",
      "Seed:  ['COLING', 'EMNLP', 'SIGIR', 'CIKM', 'CoRR']\n",
      "size:  10 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['ECIR', 'highly senior', 'prolific', 'CIKM', 'CoRR'] 10 0\n",
      "1 :  ['senior', 'CEUR Workshop Proceedings', 'CIKM', 'male', 'CoRR'] 10 0\n",
      "2 :  ['KDD', 'very productive', 'CoRR'] 62 1\n",
      "3 :  ['NIPS', 'prolific', 'CoRR'] 53 0\n",
      "4 :  ['Future Generation Comp. Syst.'] 51 0\n",
      "Seed:  ['senior', 'CEUR Workshop Proceedings', 'CIKM', 'male', 'CoRR']\n",
      "size:  10 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['CEUR Workshop Proceedings', 'AAAI', 'CIKM', 'CoRR'] 10 0\n",
      "1 :  ['senior', 'CEUR Workshop Proceedings', 'CIKM', 'male', 'CoRR'] 10 0\n",
      "2 :  ['WSDM', 'CEUR Workshop Proceedings', 'CIKM', 'male', 'CoRR'] 11 0\n",
      "3 :  ['CEUR Workshop Proceedings', 'confirmed', 'CIKM', 'male', 'CoRR'] 11 0\n",
      "4 :  ['CEUR Workshop Proceedings', 'highly senior', 'CIKM', 'CoRR'] 11 0\n",
      "Seed:  ['CEUR Workshop Proceedings', 'confirmed', 'CIKM', 'male', 'CoRR']\n",
      "size:  11 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['senior', 'CEUR Workshop Proceedings', 'CIKM', 'male', 'CoRR'] 10 0\n",
      "1 :  ['CEUR Workshop Proceedings', 'AAAI', 'CIKM', 'CoRR'] 10 0\n",
      "2 :  ['CEUR Workshop Proceedings', 'WWW (Companion Volume)', 'Europe', 'CIKM', 'CoRR'] 10 0\n",
      "3 :  ['CEUR Workshop Proceedings', 'confirmed', 'CIKM', 'male', 'CoRR'] 11 0\n",
      "4 :  ['CEUR Workshop Proceedings', 'highly senior', 'CIKM', 'male', 'CoRR'] 10 0\n",
      "Seed:  ['CEUR Workshop Proceedings', 'AAAI', 'CIKM', 'CoRR']\n",
      "size:  10 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by_distribution\n",
      "Display:\n",
      "0 :  ['very productive', 'CoRR'] 412 8\n",
      "1 :  ['Europe', 'prolific', 'CIKM', 'CoRR'] 51 1\n",
      "2 :  ['Asia', 'IEEE Trans. Knowl. Data Eng.', 'prolific', 'male', 'CoRR'] 46 1\n",
      "3 :  ['ICDM', 'prolific', 'North America', 'CoRR'] 39 0\n",
      "4 :  ['NeurIPS', 'NIPS', 'CoRR'] 38 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  ['very productive', 'CoRR']\n",
      "size:  412 , targets:  8\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['ECIR', 'highly senior', 'Lecture Notes in Computer Science', 'male', 'CoRR'] 11 0\n",
      "1 :  ['senior', 'ECIR', 'prolific', 'CoRR'] 10 0\n",
      "2 :  ['ICWSM', 'WWW (Companion Volume)', 'Europe', 'prolific', 'CoRR'] 10 0\n",
      "3 :  ['ICWSM', 'very productive', 'WWW (Companion Volume)', 'male', 'CoRR'] 10 0\n",
      "4 :  ['ICWSM', 'WWW', 'North America', 'confirmed', 'CoRR'] 16 0\n",
      "Seed:  ['ECIR', 'highly senior', 'Lecture Notes in Computer Science', 'male', 'CoRR']\n",
      "size:  11 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['ECIR', 'Lecture Notes in Computer Science', 'Europe', 'CIKM', 'CoRR'] 10 0\n",
      "1 :  ['ECIR', 'highly senior', 'Lecture Notes in Computer Science', 'CIKM', 'CoRR'] 10 0\n",
      "2 :  ['ECIR', 'CEUR Workshop Proceedings', 'Lecture Notes in Computer Science', 'CoRR'] 11 0\n",
      "3 :  ['ECIR', 'CEUR Workshop Proceedings', 'Lecture Notes in Computer Science', 'male', 'CoRR'] 10 0\n",
      "4 :  ['ECIR', 'Lecture Notes in Computer Science', 'SIGIR', 'male', 'CoRR'] 18 0\n",
      "Seed:  ['ECIR', 'highly senior', 'Lecture Notes in Computer Science', 'CIKM', 'CoRR']\n",
      "size:  10 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['ECIR', 'Lecture Notes in Computer Science', 'Europe', 'CIKM', 'CoRR'] 10 0\n",
      "1 :  ['ECIR', 'Lecture Notes in Computer Science', 'CIKM', 'CoRR'] 21 0\n",
      "2 :  ['ECIR', 'Lecture Notes in Computer Science', 'SIGIR', 'CIKM', 'CoRR'] 17 0\n",
      "3 :  ['ECIR', 'Lecture Notes in Computer Science', 'prolific', 'CIKM', 'CoRR'] 15 0\n",
      "4 :  ['ECIR', 'highly senior', 'Lecture Notes in Computer Science', 'CIKM', 'CoRR'] 10 0\n",
      "Seed:  ['ECIR', 'Lecture Notes in Computer Science', 'SIGIR', 'CIKM', 'CoRR']\n",
      "size:  17 , targets:  0\n",
      "Reward:  0\n",
      "Action:  by-topic\n",
      "Display:\n",
      "0 :  ['EVIA@NTCIR', 'SIGIR'] 16 0\n",
      "1 :  ['Lecture Notes in Business Information Processing'] 13 0\n",
      "2 :  ['CEUR Workshop Proceedings', 'highly senior', 'WWW'] 11 0\n",
      "3 :  ['very active', 'Encyclopedia of Database Systems (2nd ed.)'] 20 1\n",
      "4 :  ['Inf. Syst.', 'Lecture Notes in Computer Science', 'CIKM'] 17 0\n",
      "Seed:  ['very active', 'Encyclopedia of Database Systems (2nd ed.)']\n",
      "size:  20 , targets:  1\n",
      "Reward:  0\n",
      "Action:  by-conference\n",
      "Display:\n",
      "0 :  ['BigData', 'highly senior', 'North America', 'Encyclopedia of Database Systems (2nd ed.)'] 11 0\n",
      "1 :  ['very productive', 'North America', 'Encyclopedia of Database Systems (2nd ed.)', 'confirmed', 'SIGMOD Conference'] 10 2\n",
      "2 :  ['very productive', 'AAAI', 'Encyclopedia of Database Systems (2nd ed.)', 'CoRR'] 10 1\n",
      "3 :  ['SAC', 'Europe', 'Encyclopedia of Database Systems (2nd ed.)'] 10 0\n",
      "4 :  ['female', 'prolific', 'Encyclopedia of Database Systems (2nd ed.)', 'PVLDB'] 10 1\n",
      "Seed:  ['very productive', 'North America', 'Encyclopedia of Database Systems (2nd ed.)', 'confirmed', 'SIGMOD Conference']\n",
      "size:  10 , targets:  2\n",
      "Reward:  1\n",
      "[1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 0, 1, 0, 0, 0, 5, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print a session\n",
    "new_policy = Policy(init_weights = list(new_policy.weights[:]), eps = 0.00, alpha = 0.0, gamma = 0.0)\n",
    "rewards, history = semi_gradient_sarsa(test_environment, new_policy, episodes_num = 1, episode_length = 10, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for the reward constant C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "PC = testPC\n",
    "num_seen_users = []\n",
    "num_seen_targets = []\n",
    "num_target_groups = []\n",
    "num_targets = len(PC)\n",
    "C = np.arange(0,0.4,0.005)\n",
    "\n",
    "for c in tqdm(C):\n",
    "    discoverable_targets = set()\n",
    "    num_g = 0\n",
    "    for g in all_groups:\n",
    "        targets_in_g = intersect(users_list[g], PC)\n",
    "        if len(targets_in_g)/supports_list[g] > c:#*num_targets/supports_list[0]:\n",
    "            discoverable_targets = discoverable_targets | set(users_list[g])\n",
    "            num_g += 1\n",
    "    num_target_groups.append(num_g)        \n",
    "    num_seen_users.append(len(discoverable_targets))\n",
    "    num_seen_targets.append(len(intersect(discoverable_targets,PC)))\n",
    "    print(num_seen_users[-1], num_seen_targets[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pickles/WebDB2018_num_users.pickle','wb') as f:\n",
    "    pickle.dump(num_seen_users, f)\n",
    "    \n",
    "with open('pickles/WebDB2018_num_targets.pickle','wb') as f:\n",
    "    pickle.dump(num_seen_targets, f)\n",
    "    \n",
    "with open('pickles/WebDB2018_num_target_groups.pickle','wb') as f:\n",
    "    pickle.dump(num_target_groups, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text for topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 93.8 ms, total: 1.48 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_pb_by_user_id = {}\n",
    "pb_list = []\n",
    "\n",
    "with open('csv/publications.csv') as fin:\n",
    "    fin.readline()\n",
    "    r = 0\n",
    "    for line in fin:\n",
    "        parts = line.split(',')\n",
    "        id_ = int(parts[0])\n",
    "        \n",
    "        if id_ in user_id_mapping:\n",
    "            id_ = user_id_mapping[id_ ]\n",
    "            conf = parts[2].strip(' ')\n",
    "\n",
    "            if id_ in all_pb_by_user_id.keys():\n",
    "                all_pb_by_user_id[id_] += str(r) + ' '\n",
    "                pb_list.append(r)\n",
    "            else:\n",
    "                all_pb_by_user_id[id_] = str(r) + ' '\n",
    "                pb_list.append(r)\n",
    "            r += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 67 ms, total: 1.56 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pb_by_user_id = {}\n",
    "\n",
    "with open('csv/publications.csv') as fin:\n",
    "    fin.readline()\n",
    "    r = 0\n",
    "    for line in fin:\n",
    "        parts = line.split(',')\n",
    "        id_ = int(parts[0])\n",
    "        \n",
    "        if id_ in user_id_mapping:\n",
    "            id_ = user_id_mapping[id_]\n",
    "            conf = parts[2].strip(' ')\n",
    "\n",
    "            if id_ in pb_by_user_id.keys():\n",
    "                if conf in pb_by_user_id[id_].keys():\n",
    "                    pb_by_user_id[id_][conf] += str(r) + ' '\n",
    "                else: \n",
    "                    pb_by_user_id[id_][conf] = str(r) + ' '\n",
    "            else:\n",
    "                pb_by_user_id[id_] = {}\n",
    "                pb_by_user_id[id_][conf] = str(r) + ' '\n",
    "                \n",
    "            r += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33302/33302 [00:04<00:00, 7395.15it/s] \n"
     ]
    }
   ],
   "source": [
    "G = range(0, len(users_list))\n",
    "texts = {}\n",
    "\n",
    "with open('context/text_processing/groupid_paperid.csv', 'w') as fout:\n",
    "    fout.write('group_id;items;users;paper_ids\\n')\n",
    "    for g in tqdm(G):\n",
    "        texts[g] = ''\n",
    "        num_confs = len(conference_list[g])\n",
    "        for userid in users_list[g]:\n",
    "            for conf in conference_list[g]:\n",
    "                texts[g] = texts[g] + pb_by_user_id[userid][conf]\n",
    "            if num_confs == 0:\n",
    "                texts[g] = texts[g] + all_pb_by_user_id[userid]\n",
    "\n",
    "        fout.write(str(g)+';'+str(items_list[g])+';'+str(users_list[g])+';'+texts[g][:-1]+'\\n')\n",
    "        \n",
    "        texts[g] = ''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "0abed17141ba4cb7adec415ae6d07c6d": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
